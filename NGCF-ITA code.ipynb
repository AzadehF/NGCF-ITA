{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn import MSELoss\n",
    "from torch.nn import Module\n",
    "from torch.nn import init\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix, vstack\n",
    "from scipy import sparse\n",
    "from scipy.spatial.distance import cdist, cosine\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import diag\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "from os import path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        self.user_mapper = {}\n",
    "        self.item_mapper = {}\n",
    "        self.user_counter = 0\n",
    "        self.item_counter = 0\n",
    "\n",
    "    def load_ratings_train(self, file_path):\n",
    "        df = pd.read_csv(file_path, sep=\" \", header=None)\n",
    "        df.columns = ['userId', 'itemId', 'rating']\n",
    "        \n",
    "        df, self.user_mapper, self.item_mapper, self.user_counter, self.item_counter = self._map_entities(df)\n",
    "        \n",
    "        return df, self.user_counter, self.item_counter\n",
    "\n",
    "    def load_ratings_test(self, file_path):\n",
    "        df = pd.read_csv(file_path, sep=\" \", header=None)\n",
    "        df.columns = ['userId', 'itemId', 'rating']\n",
    "        \n",
    "        df['userId'] = df['userId'].map(self.user_mapper)\n",
    "        df['itemId'] = df['itemId'].map(self.item_mapper)\n",
    "        df = df[df['userId'] < self.user_counter]\n",
    "        df = df[df['itemId'] < self.item_counter]\n",
    "        \n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def load_trust(self, file_path):\n",
    "        df = pd.read_csv(file_path, sep=\" \", header=None)\n",
    "        df.columns = ['userId1', 'userId2', 'trust']\n",
    "        \n",
    "        \n",
    "        df['userId1'] = df['userId1'].map(self.user_mapper)\n",
    "        df['userId2'] = df['userId2'].map(self.user_mapper)\n",
    "        \n",
    "        \n",
    "        df = df[df['userId1'] < self.user_counter]\n",
    "        df = df[df['userId2'] < self.user_counter]\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def load_rate_add(self, file_path):\n",
    "        df = pd.read_csv(file_path, sep=\" \", header=None)\n",
    "        df.columns = ['userId', 'itemId', 'rating']        \n",
    "        return df\n",
    "\n",
    "    def _map_entities(self, df):\n",
    "        for idx, row in df.iterrows():\n",
    "            user_id = row['userId']\n",
    "            item_id = row['itemId']\n",
    "            \n",
    "            if user_id not in self.user_mapper:\n",
    "                self.user_mapper[user_id] = self.user_counter\n",
    "                self.user_counter += 1\n",
    "            if item_id not in self.item_mapper:\n",
    "                self.item_mapper[item_id] = self.item_counter\n",
    "                self.item_counter += 1\n",
    "            \n",
    "            df.at[idx, 'userId'] = self.user_mapper[user_id]\n",
    "            df.at[idx, 'itemId'] = self.item_mapper[item_id]\n",
    "            \n",
    "            df.at[idx, 'rating'] = abs(row['rating'])\n",
    "        \n",
    "        return df, self.user_mapper, self.item_mapper, self.user_counter, self.item_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the DataLoader class\n",
    "data_loader = DataLoader()\n",
    "\n",
    "# Load train data\n",
    "train_data, userNum, itemNum = data_loader.load_ratings_train(file_path='.../train.txt')\n",
    "\n",
    "# Load test data\n",
    "test_data = data_loader.load_ratings_test(file_path='.../test.txt')\n",
    "\n",
    "# Load trust values between users\n",
    "trust_df = data_loader.load_trust(file_path='.../trust.txt')\n",
    "\n",
    "# Load additional rating data\n",
    "rate_add_df = data_loader.load_rate_add(file_path='.../rate_add.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import vstack\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GNNLayer(Module):\n",
    "\n",
    "    def __init__(self,inF,outF):\n",
    "\n",
    "        super(GNNLayer,self).__init__()\n",
    "        self.inF = inF\n",
    "        self.outF = outF\n",
    "        \n",
    "        self.linear = torch.nn.Linear(in_features=inF,out_features=outF)\n",
    "          \n",
    "        self.interActTransform = torch.nn.Linear(in_features=inF,out_features=outF)\n",
    "        \n",
    "        self.interAttention_main = torch.nn.Linear(in_features=inF,out_features=outF)\n",
    "        self.interAttention_add = torch.nn.Linear(in_features=inF,out_features=outF)\n",
    "        self.interAttention_trust = torch.nn.Linear(in_features=inF,out_features=outF)\n",
    "\n",
    "        self.a_main = nn.Parameter(torch.empty(size=(outF, 1)))\n",
    "        self.a_add = nn.Parameter(torch.empty(size=(outF, 1)))\n",
    "        self.a_trust = nn.Parameter(torch.empty(size=(outF, 1)))\n",
    "        self.a_aug = nn.Parameter(torch.empty(size=(outF, 1)))\n",
    "\n",
    "       \n",
    "\n",
    "    def forward(self, laplacianMat,selfLoop,Trust_LaplacianMat, Addrate_LaplacianMat, features):\n",
    "       \n",
    "        L1 = laplacianMat + selfLoop\n",
    "        L1 = L1.cuda()\n",
    "        L2 = laplacianMat.cuda()\n",
    "        \n",
    "\n",
    "       \n",
    "        \n",
    "        L3 = Trust_LaplacianMat+selfLoop\n",
    "        L3 = L3_1.cuda()\n",
    "        L4 = Trust_LaplacianMat.cuda()\n",
    "        \n",
    "        L5 = Addrate_LaplacianMat+selfLoop\n",
    "        L5 = L6_1.cuda()\n",
    "        L6 = Addrate_LaplacianMat.cuda()\n",
    "        \n",
    "\n",
    "        \n",
    "        inter_feature = torch.mul(features,features)\n",
    "        x = torch.sparse.mm(L1,features)\n",
    "        inter_part_main1 = self.linear(x)\n",
    "        inter_part_main2 = self.interActTransform(torch.sparse.mm(L2,inter_feature)) \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        inter_part_main3 = torch.mm(torch.tanh(self.interAttention_main(inter_part_main1+inter_part_main2)),self.a_main )\n",
    "        w_main_sum = torch.sum(inter_part_main3)/len(inter_feature)\n",
    "\n",
    "        \n",
    "        inter_part_trust = self.linear(torch.sparse.mm(L3,features))\n",
    "        inter_part_trust2 = self.interActTransform(torch.sparse.mm(L4,inter_feature)) \n",
    "        \n",
    "        inter_part_trust3 = torch.mm(torch.tanh(self.interAttention_trust(inter_part_trust+inter_part_trust2)),self.a_trust )\n",
    "        w_trust_sum = torch.sum(inter_part_trust3)/len(inter_feature)\n",
    "\n",
    "        \n",
    "        inter_part_add = self.linear(torch.sparse.mm(L5,features))\n",
    "        inter_part_add2 = self.interActTransform(torch.sparse.mm(L6,inter_feature)) \n",
    "            \n",
    "        \n",
    "        inter_part_add3 = torch.mm(torch.tanh(self.interAttention_add(inter_part_add+inter_part_add2)),self.a_add )\n",
    "        w_add_sum = torch.sum(inter_part_add3)/len(inter_feature)\n",
    "         \n",
    "        W = torch.randn(1, 3)\n",
    "        W[0,0] = w_main_sum\n",
    "        W[0,1] = w_add_sum\n",
    "        W[0,2] = w_trust_sum\n",
    "        m = nn.Softmax(dim=1)\n",
    "        Beta = m(W)\n",
    "\n",
    "\n",
    "\n",
    "        return  (Beta[0,0]) * inter_part_main1 + ((Beta[0,1]) * inter_part_add) + ((Beta[0,2]) * inter_part_trust) + ((Beta[0,0]) *  inter_part_main2) + ((Beta[0,1]) *inter_part_add2) + ((Beta[0,2]) * inter_part_trust2) # imputation + trust\n",
    "        \n",
    "           \n",
    "\n",
    "class GCF(Module):\n",
    "\n",
    "    def __init__(self,userNum,itemNum,rt, trust, rt_add, embedSize=100,layers=[100,80,50],useCuda=False):\n",
    "        super(GCF,self).__init__()\n",
    "        \n",
    "        self.useCuda = useCuda\n",
    "        self.userNum = userNum\n",
    "        self.itemNum = itemNum\n",
    "        self.uEmbd = nn.Embedding(userNum,embedSize)\n",
    "        self.iEmbd = nn.Embedding(itemNum,embedSize)\n",
    "        self.GNNlayers = torch.nn.ModuleList()\n",
    "        self.LaplacianMat, self.Trust_LaplacianMat, self.Addrate_LaplacianMat = self.buildLaplacianMat(rt,trust, rt_add, 1)\n",
    "        \n",
    "        self.leakyRelu = nn.LeakyReLU()\n",
    "        self.selfLoop = self.getSparseEye(self.userNum+self.itemNum)\n",
    "\n",
    "        self.transForm1 = nn.Linear(in_features=layers[-1]*2,out_features=64)\n",
    "        self.transForm2 = nn.Linear(in_features=64,out_features=32)\n",
    "        self.transForm3 = nn.Linear(in_features=32,out_features=1)\n",
    "\n",
    "        for From,To in zip(layers[:-1],layers[1:]):\n",
    "            self.GNNlayers.append(GNNLayer(From,To))\n",
    "\n",
    "    def getSparseEye(self,num):\n",
    "        i = torch.LongTensor([[k for k in range(0,num)],[j for j in range(0,num)]])\n",
    "        val = torch.FloatTensor([1]*num)\n",
    "        return torch.sparse.FloatTensor(i,val)\n",
    "\n",
    "    def buildLaplacianMat(self,rt,trust, rt_add, finalEmbd):\n",
    "\n",
    "        rt_item = rt['itemId'] + self.userNum\n",
    "        uiMat = coo_matrix((rt['rating'], (rt['userId'], rt['itemId'])))\n",
    "        uiMat_upperPart = coo_matrix((rt['rating'], (rt['userId'], rt_item)))\n",
    "        uiMat = uiMat.transpose()\n",
    "        uiMat.resize((self.itemNum, self.userNum + self.itemNum))\n",
    "\n",
    "        A = sparse.vstack([uiMat_upperPart,uiMat])\n",
    "        \n",
    "        \n",
    "        uiMat_upperPart_rtadd=sparse.dok_matrix((self.userNum,self.userNum + self.itemNum), dtype=np.float32) \n",
    "        uiMat_rtadd=sparse.dok_matrix((self.itemNum,self.userNum + self.itemNum), dtype=np.float32) \n",
    "\n",
    "        \n",
    "        rtadd_item = rt_add['itemId'] + self.userNum\n",
    "        uiMat_rtadd = coo_matrix((rt_add['rating'], (rt_add['userId'], rt_add['itemId'])))\n",
    "        uiMat_upperPart_rtadd = coo_matrix((rt_add['rating'], (rt_add['userId'], rtadd_item)))\n",
    "        uiMat_rtadd = uiMat_rtadd.transpose()\n",
    "        uiMat_rtadd.resize((self.itemNum, self.userNum + self.itemNum))\n",
    "        uiMat_upperPart_rtadd.resize((self.userNum, self.userNum + self.itemNum))\n",
    "\n",
    "        A_rtadd = sparse.vstack([uiMat_upperPart_rtadd,uiMat_rtadd])\n",
    "\n",
    "        \n",
    "        \n",
    "        ui_trust_tmp=sparse.dok_matrix((self.userNum,self.userNum + self.itemNum), dtype=np.float32) \n",
    "        iu_sim_tmp=sparse.dok_matrix((self.itemNum,self.userNum + self.itemNum), dtype=np.float32) \n",
    "        \n",
    "        u_trust_idx = torch.LongTensor([i for i in range(self.userNum)]) \n",
    "        i_sim_idx = torch.LongTensor([i for i in range(self.itemNum)])+ self.userNum\n",
    "\n",
    "        if self.useCuda == True:\n",
    "            u_trust_idx = u_trust_idx.cuda()\n",
    "            i_sim_idx = i_sim_idx.cuda()\n",
    "\n",
    "        for i in range(len(trust)):\n",
    "            if trust.iloc[i,0] < self.userNum and trust.iloc[i,1] < self.userNum:\n",
    "                ui_trust_tmp[trust.iloc[i,0],trust.iloc[i,1]]=trust.iloc[i,2]\n",
    "    \n",
    "        A_trust = sparse.vstack([ui_trust_tmp,iu_sim_tmp])\n",
    "        \n",
    "\n",
    "        selfLoop = sparse.eye(self.userNum+self.itemNum)\n",
    "        sumArr = (A>0).sum(axis=1)\n",
    "        diag = list(np.array(sumArr.flatten())[0])\n",
    "        diag = np.power(diag,-0.5)\n",
    "        D = sparse.diags(diag)\n",
    "        L = D * A * D\n",
    "        L = sparse.coo_matrix(L)\n",
    "        row = L.row\n",
    "        col = L.col\n",
    "        i = torch.LongTensor([row,col])\n",
    "        data = torch.FloatTensor(L.data)\n",
    "        SparseA = torch.sparse.FloatTensor(i,data)\n",
    "\n",
    "       \n",
    "        sumArr = (A_trust>0).sum(axis=1)\n",
    "        diag = list( 1/np.array(sumArr.flatten())[0]) \n",
    "        diag = np.power(diag,-0.5)\n",
    "        D = sparse.diags(diag)\n",
    "        L = D * A_trust * D\n",
    "        L = sparse.coo_matrix(L)\n",
    "        row = L.row\n",
    "        col = L.col\n",
    "        i = torch.LongTensor([row,col])\n",
    "        data = torch.FloatTensor(L.data)\n",
    "        SparseA_trust = torch.sparse.FloatTensor(i,data,torch.Size(L.shape))\n",
    "        print(SparseA_trust.shape)\n",
    "\n",
    "        sumArr =  (A_rtadd>0).sum(axis=1)\n",
    "        diag = list( np.array(sumArr.flatten())[0])\n",
    "        diag = np.power(diag,-0.5)\n",
    "        D = sparse.diags(diag)\n",
    "        L = D * A_rtadd * D\n",
    "        L = sparse.coo_matrix(L)\n",
    "        row = L.row\n",
    "        col = L.col\n",
    "        i = torch.LongTensor([row,col])\n",
    "        data = torch.FloatTensor(L.data)\n",
    "        SparseA_rtadd = torch.sparse.FloatTensor(i,data,torch.Size(L.shape))\n",
    "\n",
    "        return SparseA, SparseA_trust, SparseA_rtadd\n",
    "\n",
    "    def getFeatureMat(self):\n",
    "        uidx = torch.LongTensor([i for i in range(self.userNum)])\n",
    "        iidx = torch.LongTensor([i for i in range(self.itemNum)])\n",
    "        if self.useCuda == True:\n",
    "            uidx = uidx.cuda()\n",
    "            iidx = iidx.cuda()\n",
    "\n",
    "        userEmbd = self.uEmbd(uidx)\n",
    "        itemEmbd = self.iEmbd(iidx)\n",
    "        \n",
    "        features = torch.cat([userEmbd,itemEmbd],dim=0)\n",
    "        return features\n",
    "\n",
    "    def forward(self,userIdx,itemIdx):\n",
    "\n",
    "        itemIdx = itemIdx + self.userNum\n",
    "        userIdx = list(userIdx.cpu().data)\n",
    "        itemIdx = list(itemIdx.cpu().data)\n",
    "        \n",
    "        features = self.getFeatureMat()\n",
    "        Embeddings = features.clone()\n",
    "        \n",
    "        all_embeddings = [Embeddings]\n",
    "        \n",
    "        for gnn in self.GNNlayers:\n",
    "            features = gnn(self.LaplacianMat,self.selfLoop ,self.Trust_LaplacianMat, self.Addrate_LaplacianMat, features)\n",
    "            all_embeddings += [features.clone()]\n",
    "            \n",
    "        all_embeddings = torch.stack(all_embeddings, dim=1)\n",
    "        final_embeddings = torch.mean(all_embeddings, dim=1)\n",
    "        \n",
    "\n",
    "        userEmbd = final_embeddings[userIdx]\n",
    "        itemEmbd = final_embeddings[itemIdx]\n",
    "        \n",
    "        embd = torch.cat([userEmbd,itemEmbd],dim=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        embd = nn.ReLU()(self.transForm1(embd))\n",
    "       \n",
    "        embd = self.transForm2(embd)\n",
    "        embd = self.transForm3(embd)\n",
    "        prediction = embd.flatten()\n",
    "\n",
    "        return prediction,userEmbd,itemEmbd,final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ML1K(Dataset):\n",
    "\n",
    "    def __init__(self,rt):\n",
    "        super(Dataset,self).__init__()\n",
    "        self.uId = list(rt['userId'])\n",
    "        self.iId = list(rt['itemId'])\n",
    "        self.rt = list(rt['rating'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.uId)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return (self.uId[item],self.iId[item],self.rt[item])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "para = {\n",
    "    'epoch':300,\n",
    "    'lr':0.001,\n",
    "    'batch_size':2048,\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "train = ML1K(train_data)\n",
    "test = ML1K(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
  
    "reg_lambda = 0.001\n",
    "k = 5\n",
    "\n",
    "dl = DataLoader(train,batch_size=para['batch_size'],shuffle=False,pin_memory=True)\n",
    "\n",
    "model = GCF(userNum, itemNum, train_data, trust_df,rate_add_df, 80, layers=[80,80,80,])#.cuda()\n",
    "optim = Adam(model.parameters(), lr=para['lr'])\n",
    "lossfn = MSELoss()\n",
    "testdl = DataLoader(test,batch_size=len(test),)\n",
    "\n",
    "best_rmse = 1500\n",
    "best_mae = 1500\n",
    "\n",
    "for param in model.parameters():\n",
    "    if param.dim() > 1:  # Only apply initialization to weights, not biases\n",
    "        nn.init.xavier_uniform_(param)\n",
    "\n",
    "start = datetime.now()\n",
    "for i in range(para['epoch']):\n",
    "\n",
    "    for id, batch in enumerate(dl):\n",
    "        optim.zero_grad()\n",
    "        prediction, userEmbd, itemEmbd, finalEmbd = model(batch[0], batch[1])\n",
    "        prediction_list = list(prediction)\n",
    "        loss = lossfn(batch[2].float(), prediction)\n",
    "\n",
    "        l2_lambda = reg_lambda\n",
    "        l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "\n",
    "        loss = loss + l2_lambda * l2_norm\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    \n",
    "    for data in testdl:\n",
    "        prediction, userEmbd, itemEmbd, finalEmbd = model(data[0], data[1])\n",
    "\n",
    "    loss = lossfn(data[2].float(), prediction)\n",
    "    MAE = np.mean(np.abs(data[2].detach().numpy() - prediction.cpu().detach().numpy()))\n",
    "\n",
    "    if np.sqrt(loss.item()) < best_rmse:\n",
    "        best_rmse = np.sqrt(loss.item())\n",
    "        rmse_epoch = i\n",
    "    if MAE < best_mae:\n",
    "        best_mae = MAE\n",
    "        mae_epoch = i   \n",
    "\n",
    "    print(' best_rmse Loss in test ', best_rmse, ' Rmse_epoch ', rmse_epoch, ' best_mae Loss in test  ', best_mae, ' mae_epoch ', mae_epoch)\n",
    "    print('Epoch ', i, ' RMSE Loss in test *** ', np.sqrt(loss.item()), ' MAE Loss in test ', MAE)\n",
    "\n",
    "end_time = datetime.now()\n",
    "\n",
    "print('Time :',end_time-start,' Final Best RMSE ',best_rmse,' mae ',best_mae,' iteration_loss ',rmse_epoch,' iteration_mae ',mae_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
